{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cLJsY7dX7pA",
        "outputId": "701f8b67-77a8-4ae6-d952-85557b1085d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Diverse_specific_clarification_questions'...\n",
            "remote: Enumerating objects: 306, done.\u001b[K\n",
            "remote: Counting objects: 100% (306/306), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 306 (delta 73), reused 298 (delta 71), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (306/306), 16.59 MiB | 19.33 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LokeshJatangi/Diverse_specific_clarification_questions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Diverse_specific_clarification_questions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuQAJ6AvX9PQ",
        "outputId": "73426174-3df8-4df2-8b0d-a23b246ef05b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Diverse_specific_clarification_questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3BurMDnYVmE",
        "outputId": "6c9d55d0-2356-4b2b-bd33-99bba5994293"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (4.63.0)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->-r requirements.txt (line 10)) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (0.9.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 7)) (1.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 7)) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2021.10.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r requirements.txt (line 9)) (2019.12.20)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r requirements.txt (line 9)) (0.8.9)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.4.0 sacrebleu-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/4587504/files/KPCNet_resources.zip.001\n",
        "!wget https://zenodo.org/record/4587504/files/KPCNet_resources.zip.002\n",
        "!wget https://zenodo.org/record/4587504/files/KPCNet_resources.zip.003\n",
        "!wget https://zenodo.org/record/4587504/files/KPCNet_resources.zip.004\n",
        "\n",
        "!cat KPCNet_resources.zip.00* > KPCNet_resources.zip\n",
        "!unzip KPCNet_resources.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il7fkWKvYDMY",
        "outputId": "014cc47f-161b-425d-842e-51be64303683"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-01 18:49:51--  https://zenodo.org/record/4587504/files/KPCNet_resources.zip.001\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67108864 (64M) [application/octet-stream]\n",
            "Saving to: ‘KPCNet_resources.zip.001’\n",
            "\n",
            "KPCNet_resources.zi 100%[===================>]  64.00M  19.2MB/s    in 3.3s    \n",
            "\n",
            "2022-04-01 18:49:56 (19.2 MB/s) - ‘KPCNet_resources.zip.001’ saved [67108864/67108864]\n",
            "\n",
            "--2022-04-01 18:49:56--  https://zenodo.org/record/4587504/files/KPCNet_resources.zip.002\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67108864 (64M) [application/octet-stream]\n",
            "Saving to: ‘KPCNet_resources.zip.002’\n",
            "\n",
            "KPCNet_resources.zi 100%[===================>]  64.00M  16.0MB/s    in 4.0s    \n",
            "\n",
            "2022-04-01 18:50:02 (16.0 MB/s) - ‘KPCNet_resources.zip.002’ saved [67108864/67108864]\n",
            "\n",
            "--2022-04-01 18:50:02--  https://zenodo.org/record/4587504/files/KPCNet_resources.zip.003\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67108864 (64M) [application/octet-stream]\n",
            "Saving to: ‘KPCNet_resources.zip.003’\n",
            "\n",
            "KPCNet_resources.zi 100%[===================>]  64.00M  17.4MB/s    in 3.7s    \n",
            "\n",
            "2022-04-01 18:50:07 (17.4 MB/s) - ‘KPCNet_resources.zip.003’ saved [67108864/67108864]\n",
            "\n",
            "--2022-04-01 18:50:07--  https://zenodo.org/record/4587504/files/KPCNet_resources.zip.004\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33322567 (32M) [application/octet-stream]\n",
            "Saving to: ‘KPCNet_resources.zip.004’\n",
            "\n",
            "KPCNet_resources.zi 100%[===================>]  31.78M  14.7MB/s    in 2.2s    \n",
            "\n",
            "2022-04-01 18:50:12 (14.7 MB/s) - ‘KPCNet_resources.zip.004’ saved [33322567/33322567]\n",
            "\n",
            "Archive:  KPCNet_resources.zip\n",
            "   creating: ckpt/\n",
            "  inflating: ckpt/s2s_D0.3_cnn_noneg_dropout_replace_fr.epoch59.models  \n",
            "   creating: data/\n",
            "  inflating: data/kwd_edges.npz      \n",
            "  inflating: data/kwd_filter_dict.json  \n",
            "  inflating: data/test.kwds          \n",
            "  inflating: data/test_ans.txt       \n",
            "  inflating: data/test_asin.txt      \n",
            "  inflating: data/test_context.txt   \n",
            "  inflating: data/test_ques.txt      \n",
            "  inflating: data/test_ref.kwds      \n",
            "  inflating: data/test_ref0          \n",
            "  inflating: data/test_ref1          \n",
            "  inflating: data/test_ref2          \n",
            "  inflating: data/test_ref3          \n",
            "  inflating: data/test_ref4          \n",
            "  inflating: data/test_ref5          \n",
            "  inflating: data/test_ref6          \n",
            "  inflating: data/test_ref7          \n",
            "  inflating: data/test_ref8          \n",
            "  inflating: data/test_ref9          \n",
            "  inflating: data/test_ref_combined  \n",
            "  inflating: data/train.kwds         \n",
            "  inflating: data/train_ans.txt      \n",
            "  inflating: data/train_asin.txt     \n",
            "  inflating: data/train_context.txt  \n",
            "  inflating: data/train_kwd_vocab.txt  \n",
            "  inflating: data/train_ques.txt     \n",
            "  inflating: data/tune.kwds          \n",
            "  inflating: data/tune_ans.txt       \n",
            "  inflating: data/tune_asin.txt      \n",
            "  inflating: data/tune_context.txt   \n",
            "  inflating: data/tune_ques.txt      \n",
            "  inflating: data/tune_ref.kwds      \n",
            "  inflating: data/tune_ref0          \n",
            "  inflating: data/tune_ref1          \n",
            "  inflating: data/tune_ref2          \n",
            "  inflating: data/tune_ref3          \n",
            "  inflating: data/tune_ref4          \n",
            "  inflating: data/tune_ref5          \n",
            "  inflating: data/tune_ref6          \n",
            "  inflating: data/tune_ref7          \n",
            "  inflating: data/tune_ref8          \n",
            "  inflating: data/tune_ref9          \n",
            "  inflating: data/tune_ref_combined  \n",
            "  inflating: data/vocab.p            \n",
            "  inflating: data/word_embeddings.p  \n",
            "replace data_office/kwd_edges.npz? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "   creating: hparams/\n",
            "  inflating: hparams/s2s_D0.3_cnn_noneg_dropout_replace_fr.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle as p\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from read_data import *\n",
        "from process_data import *\n",
        "from run import *\n",
        "from constants import *\n",
        "from hparams import hparams\n",
        "from utils import *\n",
        "import _locale\n",
        "_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])"
      ],
      "metadata": {
        "id": "kHJ-J5ApXJFa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = p.load(open(\"./data/word_embeddings.p\", 'rb'))\n",
        "word_embeddings = np.array(word_embeddings)\n",
        "word2index = p.load(open(\"./data/vocab.p\", 'rb'))\n",
        "\n",
        "index2kwd, kwd2index, index2cnt = read_kwd_vocab(\"./data/train_kwd_vocab.txt\")\n",
        "\n",
        "index2word = reverse_dict(word2index)"
      ],
      "metadata": {
        "id": "5oxLPGXrXLcp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderRNN(hparams.HIDDEN_SIZE, word_embeddings, hparams.RNN_LAYERS,\n",
        "                         dropout=hparams.DROPOUT, update_wd_emb=hparams.UPDATE_WD_EMB)\n",
        "decoder = AttnDecoderRNN(hparams.HIDDEN_SIZE, len(word2index), word_embeddings, hparams.ATTN_TYPE,hparams.RNN_LAYERS, dropout=hparams.DROPOUT, update_wd_emb=hparams.UPDATE_WD_EMB,\n",
        "                             condition=hparams.DECODER_CONDITION_TYPE)\n",
        "kwd_predictor = get_predictor(word_embeddings, hparams)\n",
        "kwd_bridge = MLPBridge(hparams.HIDDEN_SIZE, hparams.MAX_KWD, hparams.HIDDEN_SIZE, len(word_embeddings[0]),\n",
        "                               norm_type=hparams.BRIDGE_NORM_TYPE, dropout=hparams.DROPOUT)\n",
        "if hparams.USE_CUDA:\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "    kwd_predictor.cuda()\n",
        "    kwd_bridge.cuda()"
      ],
      "metadata": {
        "id": "DyPlmHJHXYgP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = torch.load(\"./ckpt/s2s_D0.3_cnn_noneg_dropout_replace_fr.epoch59.models\")\n",
        "hparams.load(\"./hparams/s2s_D0.3_cnn_noneg_dropout_replace_fr.json\")"
      ],
      "metadata": {
        "id": "eNT--i8EXiwq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./data/kwd_filter_dict.json\", encoding=\"utf-8\") as f:\n",
        "    filter_dict = json.load(f)\n",
        "\n",
        "def make_filter_mask(post, filter_dict, kwd2index):\n",
        "    curr_kwd_filter_mask = [0 for i in range(len(kwd2index))]\n",
        "    for keys, to_filters in filter_dict.items():\n",
        "        if keys.startswith(\"@\") and keys.endswith(\"@\"):  # regex\n",
        "            if bool(re.search(keys[1:-1], post)):\n",
        "                for kwd0 in to_filters:\n",
        "                    curr_kwd_filter_mask[kwd2index[kwd0]] = -1e20\n",
        "        else:\n",
        "            for k in keys.split(\",\"):\n",
        "                if k in post:\n",
        "                    for kwd0 in to_filters:\n",
        "                        curr_kwd_filter_mask[kwd2index[kwd0]] = -1e20\n",
        "    return curr_kwd_filter_mask"
      ],
      "metadata": {
        "id": "sdGSP23ZYe7A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.load_state_dict(models[\"encoder\"])\n",
        "decoder.load_state_dict(models[\"decoder\"])\n",
        "kwd_predictor.load_state_dict(models[\"kwd_predictor\"])\n",
        "kwd_bridge.load_state_dict(models[\"kwd_bridge\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9foQyAmYh2g",
        "outputId": "859b259a-7424-409d-d3e6-e791fe16b748"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from beam import evaluate_beam"
      ],
      "metadata": {
        "id": "K9o1veUAk4Vb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(text):\n",
        "    text = re.sub(r\"& (\\S+) ;\", r\"&\\1;\", text)\n",
        "    text = re.sub(r\"& # (\\S+) ;\", r\"&#\\1;\", text)\n",
        "    text = html.unescape(text)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r\"( <EOS>)\", \"\", clean_html(text.strip()))\n",
        "\n",
        "# Jaccard\n",
        "def sent_sim(text1, text2):\n",
        "    words1, words2 = set(text1.lower().strip().split()), set(text2.lower().strip().split())\n",
        "    return len(words1 & words2) / len(words1 | words2)\n",
        "\n",
        "def deduplicate(texts0, preserve=3, threshold=0.5):\n",
        "    texts = texts0[:]\n",
        "    assert len(texts) >= preserve and preserve > 0\n",
        "    if len(texts) == preserve:\n",
        "        return list(range(len(texts))), texts\n",
        "    sel_ids, remain_ids = [0], list(range(1, len(texts)))\n",
        "    sel_texts, remain_texts = texts[:1], texts[1:]\n",
        "    for i in range(1, preserve):\n",
        "        overlaps = []\n",
        "        sel_cand = None\n",
        "        for cand_id, cand in enumerate(remain_texts):\n",
        "            overlap = max(sent_sim(cand, sel) for sel in sel_texts)\n",
        "            if overlap < threshold:\n",
        "                sel_cand = cand_id\n",
        "                break\n",
        "            overlaps.append(overlap)\n",
        "        if sel_cand is None:\n",
        "            sel_cand = np.argmin(overlaps)\n",
        "        sel_texts.append(remain_texts[sel_cand])\n",
        "        sel_ids.append(remain_ids[sel_cand])\n",
        "        del remain_texts[sel_cand]\n",
        "        del remain_ids[sel_cand]\n",
        "    return sel_ids, sel_texts\n"
      ],
      "metadata": {
        "id": "Sm9-KtNUlDPF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jNi7o6mXSNHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2731604b-3e98-42b5-aefa-64bb35580df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBATCH:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/content/Diverse_specific_clarification_questions/beam.py:223: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  if word2index[EOS_token] in prev_backtrack_seqs[b, topi[b][k]//decoder.output_size, :t]:\n",
            "/content/Diverse_specific_clarification_questions/beam.py:228: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prev_decoder_hiddens[k, :, b, :] = decoder_hiddens[topi[b][k]//decoder.output_size][:, b, :]\n",
            "/content/Diverse_specific_clarification_questions/beam.py:229: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  backtrack_seqs[b, k, :t] = prev_backtrack_seqs[b, topi[b][k]//decoder.output_size, :t]\n",
            "/content/Diverse_specific_clarification_questions/beam.py:224: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  backtrack_seqs[b, k, :t] = prev_backtrack_seqs[b, topi[b][k]//decoder.output_size, :t]\n",
            "BATCH: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what are the dimensions of this topper ? <EOS>\\n', 'what is the warranty on this topper ? <EOS>\\n', 'what are the dimensions of the bed ? <EOS>\\n', 'what are the dimensions of the mattress ? <EOS>\\n', 'what is the length of the bed ? <EOS>\\n', 'what is the height of the bed ? <EOS>\\n']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLUSTER: 100%|██████████| 1/1 [00:00<00:00, 18.93it/s]\n",
            "BATCH:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/content/Diverse_specific_clarification_questions/beam.py:223: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  if word2index[EOS_token] in prev_backtrack_seqs[b, topi[b][k]//decoder.output_size, :t]:\n",
            "/content/Diverse_specific_clarification_questions/beam.py:228: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prev_decoder_hiddens[k, :, b, :] = decoder_hiddens[topi[b][k]//decoder.output_size][:, b, :]\n",
            "/content/Diverse_specific_clarification_questions/beam.py:229: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  backtrack_seqs[b, k, :t] = prev_backtrack_seqs[b, topi[b][k]//decoder.output_size, :t]\n",
            "/content/Diverse_specific_clarification_questions/beam.py:224: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  backtrack_seqs[b, k, :t] = prev_backtrack_seqs[b, topi[b][k]//decoder.output_size, :t]\n",
            "BATCH: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what are the dimensions of this topper ? <EOS>\\n', 'what is the warranty on this topper ? <EOS>\\n', 'what are the dimensions of the bed ? <EOS>\\n', 'what are the dimensions of the mattress ? <EOS>\\n', 'what is the height of this topper ? <EOS>\\n', 'how long is the mattress ? <EOS>\\n']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BATCH: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['what is the thickness of the memory foam ? <EOS>\\n', 'is it made in the usa ? <EOS>\\n', 'what is the density of the foam ? <EOS>\\n', 'what is the thickness of the foam ? <EOS>\\n', 'is it made in china ? <EOS>\\n', 'what is the thickness of the memory foam topper ? <EOS>\\n']\n",
            "['what are the dimensions of this topper ?', 'what is the warranty on this topper ?', 'how long is the mattress ?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sent0 = \"sleep innovations 4-inch dual layer mattress topper . 10-year limited warranty . made in the usa . queen size  .\"\n",
        "words0 = sent0.strip().lower().split()[:hparams.MAX_POST_LEN]\n",
        "# batch of size 1\n",
        "input_seqs = [[word2index[x] if x in word2index else word2index[UNK_token] for x in words0 ]]\n",
        "input_lens = [len(words0)]\n",
        "test_data = [[\"id0\"],input_seqs,input_lens,[None],[None],[0],[0]]\n",
        "\n",
        "kwd_filter_mask0 = make_filter_mask(sent0, filter_dict, kwd2index)\n",
        "kwd_filter_masks = [kwd_filter_mask0]  # the mask here is for filter out kwds\n",
        "test_data[-1] = kwd_filter_masks\n",
        "\n",
        "hparams.BATCH_SIZE = 1\n",
        "out_seqs = evaluate_beam(word2index, index2word, encoder, decoder, kwd_predictor, kwd_bridge, test_data, hparams.MAX_QUES_LEN, \"./infer_out\", \"infer\", index2kwd, save_all_beam=True, infer=True)\n",
        "print(out_seqs)\n",
        "\n",
        "hparams.KWD_CLUSTERS = 2\n",
        "kwd_edge_cnt = scipy.sparse.load_npz(\"./data/kwd_edges.npz\")\n",
        "kwd_clusters = get_cluster_kwds(kwd_predictor, test_data, kwd_edge_cnt, index2kwd, kwd2index)\n",
        "\n",
        "hparams.DECODE_USE_KWD_LABEL = True\n",
        "out_seqs = []\n",
        "for i in range(hparams.KWD_CLUSTERS):\n",
        "    test_data[5] = kwd_clusters[i]\n",
        "    tmp_seqs = evaluate_beam(word2index, index2word, encoder, decoder, kwd_predictor, kwd_bridge, test_data, hparams.MAX_QUES_LEN, \"./infer_out\", \"infer\", index2kwd, save_all_beam=True, infer=True)\n",
        "    print(tmp_seqs)\n",
        "    out_seqs.extend(tmp_seqs)\n",
        "\n",
        "cleaned_out_seqs = [clean_text(x) for x in out_seqs]\n",
        "filtered_ids, filtered_texts = deduplicate(cleaned_out_seqs)\n",
        "print(filtered_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N"
      ],
      "metadata": {
        "id": "FICOlm1AdLhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}